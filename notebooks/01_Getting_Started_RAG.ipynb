{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Getting Started with RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "This notebook will guide you through building your first RAG (Retrieval-Augmented Generation) system using the GenerativeAI Starter Kit.\n",
    "\n",
    "## What is RAG?\n",
    "\n",
    "RAG combines the power of:\n",
    "- **Retrieval**: Finding relevant information from a knowledge base\n",
    "- **Generation**: Using an LLM to generate responses based on retrieved context\n",
    "\n",
    "This approach allows AI systems to access up-to-date information and provide more accurate, source-backed responses.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. Set up a vector database for document storage\n",
    "2. Add documents to your knowledge base\n",
    "3. Perform semantic search and retrieval\n",
    "4. Generate AI responses with source citations\n",
    "5. Build a complete question-answering system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ Prerequisites\n",
    "\n",
    "Before running this notebook, make sure you have:\n",
    "\n",
    "1. **Installed the GenerativeAI Starter Kit**\n",
    "   ```bash\n",
    "   python scripts/setup/install.py\n",
    "   ```\n",
    "\n",
    "2. **Set up your API keys in `.env` file**\n",
    "   ```\n",
    "   OPENAI_API_KEY=your_api_key_here\n",
    "   ```\n",
    "\n",
    "3. **Activated your virtual environment**\n",
    "   ```bash\n",
    "   source venv/bin/activate  # Linux/Mac\n",
    "   # or\n",
    "   venv\\Scripts\\activate     # Windows\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to Python path\n",
    "sys.path.append(str(Path('..') / 'src'))\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')\n",
    "\n",
    "# Import our RAG components\n",
    "from rag.pipeline import RAGPipeline\n",
    "from rag.vector_store import VectorStoreManager\n",
    "from rag.retriever import DocumentRetriever\n",
    "from rag.generator import ResponseGenerator, OpenAILLM\n",
    "\n",
    "print(\"âœ… All imports successful!\")\n",
    "print(f\"OpenAI API Key loaded: {'Yes' if os.getenv('OPENAI_API_KEY') else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ Step 1: Initialize the RAG Pipeline\n",
    "\n",
    "We'll create a RAG pipeline that uses:\n",
    "- **ChromaDB** for vector storage (runs locally, no additional setup needed)\n",
    "- **OpenAI** for text generation\n",
    "- **Sentence Transformers** for document embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RAG pipeline\n",
    "pipeline = RAGPipeline(\n",
    "    vector_store_type=\"chroma\",           # Local vector database\n",
    "    llm_provider=\"openai\",               # OpenAI for generation\n",
    "    collection_name=\"getting_started\",   # Name for our document collection\n",
    "    top_k=3,                            # Retrieve top 3 most relevant documents\n",
    "    similarity_threshold=0.5            # Minimum similarity score\n",
    ")\n",
    "\n",
    "print(\"ğŸ‰ RAG Pipeline initialized successfully!\")\n",
    "print(f\"Vector Store: {pipeline.vector_store.store_type}\")\n",
    "print(f\"LLM Model: {pipeline.llm.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š Step 2: Add Documents to Knowledge Base\n",
    "\n",
    "Let's add some sample documents about artificial intelligence topics. In a real application, these could be:\n",
    "- Company documentation\n",
    "- Research papers\n",
    "- Product manuals\n",
    "- FAQ documents\n",
    "- Any text-based knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents about AI and machine learning\n",
    "documents = [\n",
    "    \"\"\"\n",
    "    Artificial Intelligence (AI) Overview:\n",
    "    \n",
    "    Artificial Intelligence is the simulation of human intelligence in machines that are programmed \n",
    "    to think and learn like humans. AI systems can perform tasks that typically require human \n",
    "    intelligence, such as visual perception, speech recognition, decision-making, and language \n",
    "    translation. The field includes machine learning, deep learning, natural language processing, \n",
    "    computer vision, and robotics.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"\n",
    "    Machine Learning Fundamentals:\n",
    "    \n",
    "    Machine Learning (ML) is a subset of AI that enables computers to learn and improve from \n",
    "    experience without being explicitly programmed. ML algorithms build mathematical models \n",
    "    based on training data to make predictions or decisions. There are three main types:\n",
    "    \n",
    "    1. Supervised Learning: Uses labeled training data\n",
    "    2. Unsupervised Learning: Finds patterns in unlabeled data\n",
    "    3. Reinforcement Learning: Learns through interaction with environment\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"\n",
    "    Deep Learning and Neural Networks:\n",
    "    \n",
    "    Deep Learning is a specialized subset of machine learning that uses artificial neural networks \n",
    "    with multiple layers to model complex patterns in data. These networks are inspired by the \n",
    "    human brain and consist of interconnected nodes (neurons) organized in layers. Deep learning \n",
    "    has achieved breakthrough results in image recognition, natural language processing, speech \n",
    "    recognition, and game playing.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"\n",
    "    Natural Language Processing (NLP):\n",
    "    \n",
    "    NLP is a branch of AI that focuses on enabling computers to understand, interpret, and \n",
    "    generate human language. Key NLP tasks include:\n",
    "    \n",
    "    - Text classification and sentiment analysis\n",
    "    - Machine translation between languages\n",
    "    - Question answering and chatbots\n",
    "    - Text summarization and generation\n",
    "    - Named entity recognition\n",
    "    \n",
    "    Modern NLP relies heavily on transformer models like BERT, GPT, and T5.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"\n",
    "    RAG (Retrieval-Augmented Generation):\n",
    "    \n",
    "    RAG is a technique that combines information retrieval with text generation to create more \n",
    "    accurate and up-to-date AI responses. The process works in two steps:\n",
    "    \n",
    "    1. Retrieval: Search for relevant documents from a knowledge base using semantic similarity\n",
    "    2. Generation: Use retrieved context to generate informed responses with an LLM\n",
    "    \n",
    "    RAG enables AI systems to access external knowledge beyond their training data, making \n",
    "    responses more factual and reducing hallucinations.\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "# Metadata for each document (optional but useful for tracking sources)\n",
    "metadata = [\n",
    "    {\"source\": \"AI_Overview.pdf\", \"topic\": \"artificial_intelligence\", \"author\": \"AI Research Team\"},\n",
    "    {\"source\": \"ML_Guide.pdf\", \"topic\": \"machine_learning\", \"author\": \"Data Science Team\"},\n",
    "    {\"source\": \"DL_Handbook.pdf\", \"topic\": \"deep_learning\", \"author\": \"Neural Networks Lab\"},\n",
    "    {\"source\": \"NLP_Tutorial.pdf\", \"topic\": \"natural_language_processing\", \"author\": \"Language AI Team\"},\n",
    "    {\"source\": \"RAG_Paper.pdf\", \"topic\": \"retrieval_augmented_generation\", \"author\": \"AI Research Institute\"}\n",
    "]\n",
    "\n",
    "print(f\"ğŸ“„ Preparing to add {len(documents)} documents to the knowledge base...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add documents to the pipeline\n",
    "result = pipeline.add_documents(documents, metadata)\n",
    "\n",
    "print(\"âœ… Documents added successfully!\")\n",
    "print(f\"ğŸ“Š Documents added: {result['documents_added']}\")\n",
    "print(f\"ğŸ§© Chunks created: {result['chunks_created']}\")\n",
    "print(f\"â±ï¸  Processing time: {result['processing_time']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” Step 3: Test Document Retrieval\n",
    "\n",
    "Before generating responses, let's test the retrieval system to see if it can find relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test retrieval with a sample query\n",
    "test_query = \"What is machine learning?\"\n",
    "\n",
    "print(f\"ğŸ” Testing retrieval for query: '{test_query}'\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get retrieved documents\n",
    "retrieved_docs = pipeline.retriever.retrieve(test_query, k=3)\n",
    "\n",
    "print(f\"ğŸ“‹ Found {len(retrieved_docs)} relevant documents:\")\n",
    "print()\n",
    "\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    relevance_score = doc['relevance_score']\n",
    "    source = doc['metadata'].get('source', 'Unknown')\n",
    "    snippet = doc['document'][:200] + \"...\" if len(doc['document']) > 200 else doc['document']\n",
    "    \n",
    "    print(f\"ğŸ“„ Document {i}:\")\n",
    "    print(f\"   ğŸ“Š Relevance Score: {relevance_score:.3f}\")\n",
    "    print(f\"   ğŸ“ Source: {source}\")\n",
    "    print(f\"   ğŸ“ Snippet: {snippet.strip()}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤– Step 4: Generate AI Responses\n",
    "\n",
    "Now let's use the complete RAG pipeline to generate informed responses with source citations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries for our AI system\n",
    "test_queries = [\n",
    "    \"What is machine learning and how does it work?\",\n",
    "    \"What's the difference between AI and deep learning?\",\n",
    "    \"How does RAG improve AI responses?\",\n",
    "    \"What are the main applications of NLP?\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ¤– Testing RAG Pipeline with Multiple Queries\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\nâ“ Query {i}: {query}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Generate response with sources\n",
    "    response = pipeline.query(\n",
    "        query,\n",
    "        include_sources=True,    # Include source citations\n",
    "        temperature=0.7,        # Creativity level (0.0 = focused, 1.0 = creative)\n",
    "        max_tokens=300          # Maximum response length\n",
    "    )\n",
    "    \n",
    "    print(f\"ğŸ¯ **Answer:**\")\n",
    "    print(response['answer'])\n",
    "    print()\n",
    "    \n",
    "    # Show sources\n",
    "    if response.get('sources'):\n",
    "        print(f\"ğŸ“š **Sources:**\")\n",
    "        for source in response['sources']:\n",
    "            print(f\"   {source['id']} {source['name']} (Relevance: {source['relevance_score']:.3f})\")\n",
    "    \n",
    "    print(f\"â±ï¸  **Response Time:** {response['total_time']:.2f}s\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¬ Step 5: Interactive Conversation Mode\n",
    "\n",
    "Let's test the conversation mode where the AI remembers previous exchanges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset conversation history\n",
    "pipeline.reset_conversation()\n",
    "\n",
    "print(\"ğŸ’¬ Conversation Mode Example\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Simulate a conversation\n",
    "conversation = [\n",
    "    \"What is deep learning?\",\n",
    "    \"How is it different from traditional machine learning?\",\n",
    "    \"Can you give me some practical applications?\"\n",
    "]\n",
    "\n",
    "for turn, query in enumerate(conversation, 1):\n",
    "    print(f\"\\nğŸ‘¤ **Turn {turn} - User:** {query}\")\n",
    "    \n",
    "    response = pipeline.query(\n",
    "        query,\n",
    "        conversation_mode=True,  # Enable conversation context\n",
    "        include_sources=False,  # Skip sources for cleaner conversation\n",
    "        temperature=0.7,\n",
    "        max_tokens=200\n",
    "    )\n",
    "    \n",
    "    print(f\"ğŸ¤– **AI Assistant:** {response['answer']}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š **Conversation Summary:**\")\n",
    "print(f\"Total turns: {len(pipeline.conversation_history)}\")\n",
    "print(f\"Average response time: {sum(h.get('response_time', 0) for h in pipeline.conversation_history) / len(pipeline.conversation_history) if pipeline.conversation_history else 0:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 6: Pipeline Analytics\n",
    "\n",
    "Let's examine the performance and usage statistics of our RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get analytics\n",
    "analytics = pipeline.get_analytics()\n",
    "\n",
    "print(\"ğŸ“Š RAG Pipeline Analytics\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "for key, value in analytics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key.replace('_', ' ').title()}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "print(\"\\nğŸ¯ Performance Insights:\")\n",
    "if analytics.get('average_total_time', 0) < 2.0:\n",
    "    print(\"âœ… Excellent response times!\")\n",
    "elif analytics.get('average_total_time', 0) < 5.0:\n",
    "    print(\"âœ… Good response times\")\n",
    "else:\n",
    "    print(\"âš ï¸  Consider optimizing for faster responses\")\n",
    "\n",
    "if analytics.get('average_response_length', 0) > 100:\n",
    "    print(\"âœ… Generating comprehensive responses\")\n",
    "else:\n",
    "    print(\"â„¹ï¸  Responses are concise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ Step 7: Advanced Features\n",
    "\n",
    "Let's explore some advanced features of the RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Query Explanation - understand how the system processes queries\n",
    "query = \"How does neural network training work?\"\n",
    "explanation = pipeline.explain_query(query)\n",
    "\n",
    "print(f\"ğŸ” Query Analysis for: '{query}'\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Documents found: {explanation['retrieval_analysis']['total_results']}\")\n",
    "print(f\"Average relevance: {explanation['retrieval_analysis']['summary']['average_score']:.3f}\")\n",
    "print(f\"Best match score: {explanation['retrieval_analysis']['summary']['highest_score']:.3f}\")\n",
    "\n",
    "if explanation.get('suggestions'):\n",
    "    print(\"\\nğŸ’¡ Suggestions for better results:\")\n",
    "    for suggestion in explanation['suggestions']:\n",
    "        print(f\"   â€¢ {suggestion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Metadata Filtering - search within specific document types\n",
    "print(\"\\nğŸ·ï¸  Metadata Filtering Example\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Search only in documents from \"AI Research Team\"\n",
    "filtered_results = pipeline.retriever.retrieve_with_metadata_filter(\n",
    "    query=\"What is artificial intelligence?\",\n",
    "    metadata_filter={\"author\": \"AI Research Team\"},\n",
    "    k=2\n",
    ")\n",
    "\n",
    "print(f\"Found {len(filtered_results)} documents from 'AI Research Team':\")\n",
    "for doc in filtered_results:\n",
    "    print(f\"   ğŸ“„ {doc['metadata']['source']} (Score: {doc['relevance_score']:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Batch Processing - process multiple queries efficiently\n",
    "print(\"\\nâš¡ Batch Processing Example\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "batch_queries = [\n",
    "    \"What is supervised learning?\",\n",
    "    \"What is unsupervised learning?\",\n",
    "    \"What is reinforcement learning?\"\n",
    "]\n",
    "\n",
    "batch_results = pipeline.batch_query(batch_queries, include_sources=False)\n",
    "\n",
    "for i, result in enumerate(batch_results):\n",
    "    print(f\"\\nâ“ {result['question']}\")\n",
    "    print(f\"ğŸ¤– {result['answer'][:150]}...\")\n",
    "    print(f\"â±ï¸  {result['total_time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Step 8: Save and Load Pipeline State\n",
    "\n",
    "You can save the pipeline state for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pipeline state\n",
    "state_file = \"../data/pipeline_state.json\"\n",
    "pipeline.save_state(state_file)\n",
    "\n",
    "print(f\"ğŸ’¾ Pipeline state saved to: {state_file}\")\n",
    "print(f\"ğŸ“Š Saved {len(pipeline.query_history)} queries and {len(pipeline.conversation_history)} conversation turns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ‰ Congratulations!\n",
    "\n",
    "You've successfully built and tested a complete RAG system! Here's what you accomplished:\n",
    "\n",
    "âœ… **Created a vector database** for document storage  \n",
    "âœ… **Added documents** with metadata to your knowledge base  \n",
    "âœ… **Performed semantic search** to find relevant information  \n",
    "âœ… **Generated AI responses** with source citations  \n",
    "âœ… **Tested conversation mode** with context memory  \n",
    "âœ… **Analyzed performance** with built-in analytics  \n",
    "âœ… **Explored advanced features** like filtering and batch processing  \n",
    "\n",
    "## ğŸš€ Next Steps\n",
    "\n",
    "Now that you understand the basics, you can:\n",
    "\n",
    "1. **Add your own documents** - Replace sample data with your domain-specific content\n",
    "2. **Try different vector databases** - Experiment with FAISS, Pinecone, or Weaviate\n",
    "3. **Customize the LLM** - Use different models or providers\n",
    "4. **Explore multimodal RAG** - Add images and audio to your knowledge base\n",
    "5. **Build a web interface** - Create a user-friendly chat interface\n",
    "6. **Deploy to production** - Scale your system for real-world use\n",
    "\n",
    "## ğŸ“š Additional Resources\n",
    "\n",
    "- **[Advanced RAG Techniques](./02_Advanced_RAG_Techniques.ipynb)** - Learn about query expansion, reranking, and more\n",
    "- **[Multimodal AI](./03_Multimodal_AI.ipynb)** - Process images, audio, and text together\n",
    "- **[Fine-tuning Models](./04_Model_Fine_Tuning.ipynb)** - Customize models for your specific use case\n",
    "- **[Production Deployment](./05_Production_Deployment.ipynb)** - Deploy your RAG system at scale\n",
    "\n",
    "Happy building! ğŸ”¥"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}